{"cells":[{"metadata":{},"cell_type":"markdown","source":"**1.Importing the basic libraries and walking through the source directory containing the files.**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Reading the 'A million headline' dataset**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/million-headlines/abcnews-date-text.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Creating the Publish Year, Publish Month and Publish Day. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['publish_year'] = data['publish_date'].apply(lambda x:int(x/10000))\ndata['publish_month'] = data['publish_date'].apply(lambda x:int(((x)%10000)/100))\ndata['publish_day'] = data['publish_date'].apply(lambda x:((x)%10000)%100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4. Plotting Number of Headlines every year**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(data['publish_year'], facecolor='blue', alpha=0.8, rwidth = 0.5)\nplt.xlabel('Year')\nplt.ylabel('#News Headlines')\nplt.title('#News Headlines in each year')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5. Plotting Number of Headlines published every month**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(data['publish_month'],12, facecolor='blue', alpha=0.8, rwidth = 0.5)\nplt.xlabel('Month')\nplt.ylabel('#News Headlines')\nplt.title('#News Headlines in each month')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**6. Plotting Number of Headlines published by every day of month**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(data['publish_day'],31, facecolor='blue', alpha=0.8, rwidth = 0.5)\nplt.xlabel('Day')\nplt.ylabel('#News Headlines')\nplt.title('#News Headlines on each day of month')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**7. Creating text corpus for word cloud and frequency plots**"},{"metadata":{"trusted":true},"cell_type":"code","source":"corp = str()\nfor i in range(len(data['headline_text'])):\n    corp += (' ')+data['headline_text'][i]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**8. Tokenize the corpus**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nwords = nltk.word_tokenize(corp)\n#data['headline_text'][1] + (' ') + data['headline_text'][2] + data['headline_text'][3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**9. Cleaning the corpus - removing stopwords & punctuations**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nf_words = [w for w in words if not w in stop_words] \n\npunctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\nfp_words = [w for w in f_words if not w in punctuations] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**10. Creating the Frequency dictionary and frequency plots**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fd = nltk.FreqDist(fp_words)\n\ndf_fdist = pd.DataFrame.from_dict(fd, orient='index')\ndf_fdist.columns = ['Frequency']\ndf_fdist.index.name = 'Term'\n\nfreq_df = df_fdist[df_fdist['Frequency']>500]\nd = freq_df.to_dict()['Frequency']\n\n#plt.figure(figsize=(20, 8))\nfreq_df1 = df_fdist[df_fdist['Frequency']>7500]\nfreq_df1.sort_values('Frequency',ascending=False).plot(kind='bar')\n#freq_df1.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**11. Creating Word Cloud**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(32,32))\nwordcloud = WordCloud()\nwordcloud.generate_from_frequencies(frequencies=d)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**12. Cleaner function to apply on the 'headline_text' column for modeling preparation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\nimport nltk\ndef text_cleaner(text):\n    stop_words = set(stopwords.words('english'))\n    f_words = [w for w in nltk.word_tokenize(text) if not w in stop_words] \n    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n    fp_words = [w for w in f_words if not w in punctuations] \n    fp_words_stem = [ps.stem(words) for words in fp_words]\n    fp_sent = ' '.join(word for word in fp_words_stem)\n    return fp_sent\n\n#text_cleaner(data['headline_text'][1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**13. Applying cleaning function to 'headline_text'**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['headline_text_clean'] = data['headline_text'].apply(text_cleaner)\ndata['headline_text_clean'][:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**14. Implementing TF-IDF on the data with capping the number of features / token to 10000. The dimension of TF-IDF is 11,03,663 X 10000**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features=10000)\nvectorizer.fit(data['headline_text_clean'].values)\ndata_tfidf = vectorizer.transform(data['headline_text_clean'])\n\ntfidf_to_word = np.array(vectorizer.get_feature_names())\n#tfidf_to_word","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**15. Decomposing the TF-IDF matrix to smaller matrices W (11,03,663 X 50) and H (50 X 10000) using Non-Negative Matrix Factorization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import NMF\nnmf = NMF(n_components=50, solver=\"mu\")\nW = nmf.fit_transform(data_tfidf)\nH = nmf.components_\n#W.shape\n#H.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**16. Each topic is a made of collection of words. Matrix H represents each topic (50 rows/topics) and words making it (10000 columns/words). Here we are printing the top 10 words making the topic.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, topic in enumerate(H):\n    print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in tfidf_to_word[topic.argsort()[-10:]]])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**17. Each document can be represented as a collection of topics. Matrix W represents each document (10000 rows/documents subsetted here) and topics making it (50 columns/topics). Here we are printing the top 10 topics making the document.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_list = np.array([(\"Topic\" + \" \"+ str(i)) for i in range(1,51)])\nfor i,topic in enumerate(W[:10000,]):\n    print(\"Headline {}: {}\".format(i+1,\",\".join([str(x) for x in topic_list[topic.argsort()[-10:]]])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**18. Implementing t-SNE to reduce the W (50 dim vectors) to 2 dims for plotting**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\ntsne_results = tsne.fit_transform(W[:10000,])\n\ndf_subset = pd.DataFrame()\ndf_subset.head()\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf_subset = pd.DataFrame()\ndf_subset['tsne-2d-one'] = tsne_results[:,0]\ndf_subset['tsne-2d-two'] = tsne_results[:,1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**19. t-SNE option 2 for plotting **"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\nplt.scatter(x=tsne_results[:,0],y=tsne_results[:,1],alpha=0.8, c=\"y\")\n\nplt.figure(figsize=(16,10))\nx = x=tsne_results[:,0]\ny = y=tsne_results[:,1]\nplt.scatter(x,y,alpha=0.8, c=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**20. Implement k-Means. 10 clusters is fast but 100 clusters gives better results. ** "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n# Number of clusters\nkmeans = KMeans(n_clusters=10)\n# Fitting the input data\nkmeans = kmeans.fit(W)\n# Getting the cluster labels\nlabels = kmeans.predict(W)\n# Centroid values\ncentroids = kmeans.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**21. Getting results from clustering**"},{"metadata":{"trusted":true},"cell_type":"code","source":"centroids\ndata['cluster'] = labels\ndata['cluster'].value_counts()\ndata[['headline_text_clean','cluster']].sample(n=1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**22. Plotting Inertial vs Number of Clusters. If k belongs to np.arange(1,102,0), it runs k-Means with k=1,11,21,....,101. Larger k might take 15 mins+ to run. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nimport numpy as np\nsse = {}\nfor k in np.arange(1, 10, 1):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(W)\n    #data[\"clusters\"] = kmeans.labels_\n    #print(data[\"clusters\"])\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**23. Finding the cluster membership of the input query for recommendation engine. ANd filtering all the headlines belonging to that clusters. The search for recommendations would happen over the members of only this cluster and not globally.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"clust = data['cluster'][2]\nW_filter = data[data['cluster'] == clust].index\nW_filter\n\nW[W_filter].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**24. A function to find cosine similarity for a given input query with all the other elements. The top 10 similar elements would be shown as recommendations for the input element. ** "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\ndef cos_pair(X):\n    sim_x = []\n    for i in range(len(W[W_filter])):\n        sim_x.append(cosine_similarity(X.reshape(1,W.shape[1]),W[W_filter][i].reshape(1,W.shape[1])))\n    return sim_x\n        \nsim = cos_pair(W[0])\ntop10recos = sorted(range(len(sim)), key=lambda i: sim[i])[-10:]\ndata['headline_text'][top10recos]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}